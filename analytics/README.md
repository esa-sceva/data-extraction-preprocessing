# Analytics & Data Management Tools

Utility scripts for analyzing and managing extracted data on S3. These tools help compare folder structures, count characters, and synchronize missing files.

---

## Scripts Overview

### 1. `compare.py` - Compare Folder Structures

Compares two S3 folder hierarchies to identify missing or extra files.

**What it does:**
- Lists all subfolders in `data_extracted/` and `data_cleaned/`
- Compares files in common subfolders
- Identifies missing files in cleaned vs extracted
- Identifies extra files in cleaned that aren't in extracted
- Generates a detailed JSON report

**Usage:**

```bash
python compare.py
```

**Configuration** (edit in script):
```python
bucket = "esa-satcom-s3"
root_path = ""  # S3 root prefix
output_file = "comparison_results.json"
```

**Output:**
```json
{
  "subfolder_name/": {
    "total_in_extracted": 100,
    "total_in_cleaned": 98,
    "missing_in_cleaned": ["file1.md", "file2.md"],
    "extra_in_cleaned": []
  }
}
```

---

### 2. `upload_missing.py` - Sync Missing Files

Copies missing files from extracted to cleaned folders based on comparison results.

**What it does:**
- Reads `comparison_results.json` from `compare.py`
- Copies missing files from `data_extracted/` to `data_pii_removal/`
- Uses S3's native copy operation (no download/upload)
- Shows progress with tqdm

**Usage:**

```bash
# First run compare.py to generate comparison_results.json
python compare.py

# Then copy missing files
python upload_missing.py
```

**Configuration** (edit in script):
```python
bucket_name = "esa-satcom-s3"
json_file = "./comparison_results.json"
s3_root_cleaned = "data_pii_removal"  # Destination
s3_root_extracted = "data_cleaned"     # Source for missing files
```

**Note:** Handles missing files gracefully - skips if source doesn't exist.

---

### 3. `analytics.py` - Count Characters & Files

Analyzes S3 folders to count total characters and files in each subfolder.

**What it does:**
- Lists all subfolders in specified S3 prefix
- Streams through each file counting characters
- Counts total files per subfolder
- Generates analytics JSON report

**Usage:**

```bash
python analytics.py
```

**Configuration** (edit in script):
```python
bucket = "esa-satcom-s3"
root_path = ""
cleaned_prefix = "data_pii_removal/"
output_file = "chars_and_files_in_data_pii_removal.json"
```

**Output:**
```json
{
  "subfolder_name/": {
    "chars_in_cleaned": 1500000,
    "num_files": 100
  }
}
```

**Use cases:**
- Dataset statistics
- Quality metrics
- Storage analysis
- Data completeness checks

---

## Common Workflow

### Scenario: Sync Extracted Data to Cleaned Folder

```bash
# Step 1: Compare folders to find differences
python compare.py

# Step 2: Review comparison_results.json
cat comparison_results.json

# Step 3: Copy missing files
python upload_missing.py

# Step 4: Generate analytics
python analytics.py
```

---

## Prerequisites

```bash
pip install boto3 tqdm
```

**AWS Configuration:**
Ensure AWS credentials are configured:
```bash
export AWS_ACCESS_KEY_ID=your_access_key
export AWS_SECRET_ACCESS_KEY=your_secret_key
export AWS_DEFAULT_REGION=eu-west-1
```

---

## Output Files

| File | Generated By | Description |
|------|--------------|-------------|
| `comparison_results.json` | `compare.py` | Detailed file differences between folders |
| `chars_and_files_in_data_pii_removal.json` | `analytics.py` | Character counts and file statistics |

---

## Notes

- **S3 Native Operations**: Scripts use S3 API directly - no local downloads
- **Large Datasets**: Progress bars show real-time status for large operations
- **Error Handling**: Missing files are logged and skipped, not failed
- **Customizable**: Edit bucket names and prefixes directly in scripts

---

## Tips

1. **Dry Run**: Review `comparison_results.json` before running `upload_missing.py`
2. **Bucket Costs**: These operations incur S3 request costs (list, copy)
3. **Parallel Processing**: For faster processing, consider adding multithreading
4. **Regex Filtering**: Extend scripts to filter specific file patterns if needed

